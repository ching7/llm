#!/Users/chenyanan/Downloads/gitproject/llm/llm-model/venv/bin/python3
"""
Qwen 0.6B æ¨¡å‹æ¨ç†æœåŠ¡
åŸºäº FastAPI çš„æŒç»­æ¨ç†æœåŠ¡ï¼Œæ¨¡å‹åŠ è½½ä¸€æ¬¡åæŒç»­æä¾›æœåŠ¡
"""
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import os
import uvicorn
from typing import Optional

# åˆå§‹åŒ– FastAPI åº”ç”¨
app = FastAPI(
    title="Qwen 0.6B æ¨ç†æœåŠ¡",
    description="åŸºäº Qwen 0.6B æ¨¡å‹çš„æŒç»­æ¨ç†æœåŠ¡",
    version="1.0.0"
)

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹å’Œåˆ†è¯å™¨
model = None
tokenizer = None
device = "cpu"

# è¯·æ±‚æ¨¡å‹
class InferenceRequest(BaseModel):
    prompt: str
    max_new_tokens: Optional[int] = 200
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 0.9
    do_sample: Optional[bool] = True

# å“åº”æ¨¡å‹
class InferenceResponse(BaseModel):
    result: str
    status: str
    message: str

@app.on_event("startup")
def load_model():
    """å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰"""
    global model, tokenizer, device
    
    print("="*60)
    print("ğŸš€ æ­£åœ¨å¯åŠ¨ Qwen 0.6B æ¨ç†æœåŠ¡...")
    print("="*60)
    
    device = "cpu"
    print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ¨¡å‹è·¯å¾„
    # model_path = os.path.expanduser("~/.cache/modelscope/hub/models/Qwen/Qwen3-0___6B")
    model_path = "/Users/chenyanan/Downloads/gitproject/llm/llm-train/outputs/sft_results/final_model"
    
    print("\nğŸ“¥ æ­£åœ¨åŠ è½½åˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    print("ğŸ“¥ æ­£åœ¨åŠ è½½æ¨¡å‹ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype=torch.float32
    )
    model = model.to(device)
    model.eval()
    
    # è®¾ç½® pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼æœåŠ¡å·²å°±ç»ª")
    print("="*60)
    print(f"ğŸ“¡ API æ–‡æ¡£åœ°å€: http://localhost:8000/docs")
    print(f"ğŸ”— å¥åº·æ£€æŸ¥: http://localhost:8000/health")
    print("="*60)

@app.get("/health")
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "device": device
    }

@app.post("/inference", response_model=InferenceResponse)
def inference(request: InferenceRequest):
    """
    æ¨ç†æ¥å£
    
    å‚æ•°:
    - prompt: è¾“å…¥æç¤ºæ–‡æœ¬
    - max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°ï¼ˆé»˜è®¤200ï¼‰
    - temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§ï¼ˆé»˜è®¤0.7ï¼‰
    - top_p: æ ¸é‡‡æ ·å‚æ•°ï¼ˆé»˜è®¤0.9ï¼‰
    - do_sample: æ˜¯å¦å¯ç”¨é‡‡æ ·ï¼ˆé»˜è®¤Trueï¼‰
    """
    global model, tokenizer, device
    
    if model is None or tokenizer is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½ï¼Œè¯·ç¨åé‡è¯•")
    
    try:
        # ç¼–ç è¾“å…¥
        inputs = tokenizer(request.prompt, return_tensors="pt")
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # ç”Ÿæˆå›å¤
        with torch.no_grad():
            out = model.generate(
                **inputs,
                max_new_tokens=request.max_new_tokens,
                do_sample=request.do_sample,
                temperature=request.temperature,
                top_p=request.top_p,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        # è§£ç è¾“å‡º
        result = tokenizer.decode(out[0], skip_special_tokens=True)
        
        return InferenceResponse(
            result=result,
            status="success",
            message="æ¨ç†å®Œæˆ"
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ¨ç†å¤±è´¥: {str(e)}")

@app.get("/")
def root():
    """æ ¹è·¯å¾„ï¼Œè¿”å›æœåŠ¡ä¿¡æ¯"""
    return {
        "service": "Qwen 0.6B æ¨ç†æœåŠ¡",
        "status": "running",
        "endpoints": {
            "health": "/health",
            "inference": "/inference (POST)",
            "docs": "/docs"
        }
    }

if __name__ == "__main__":
    # å¯åŠ¨æœåŠ¡
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=8000,
        reload=False,  # ç”Ÿäº§ç¯å¢ƒå»ºè®®è®¾ä¸º False
        log_level="info"
    )

