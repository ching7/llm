#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM SFT æ¨¡å‹è¯„ä¼°è„šæœ¬
ç”¨äºè¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹æ€§èƒ½
æ”¯æŒä¸è®­ç»ƒè„šæœ¬ç›¸åŒçš„æ•°æ®æ ¼å¼
"""

import os
import json
import torch
import argparse
import platform
import gc
from transformers import AutoModelForCausalLM, AutoTokenizer

def is_apple_silicon():
    """æ£€æµ‹æ˜¯å¦æ˜¯ Apple Silicon (M1/M2/M3 ç­‰)"""
    try:
        machine = platform.machine()
        if machine == 'arm64':
            if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                return True
        return False
    except:
        return False

def is_intel_mac():
    """æ£€æµ‹æ˜¯å¦æ˜¯ Intel Mac"""
    try:
        machine = platform.machine()
        return machine == 'x86_64'
    except:
        return False

def load_dataset(data_path):
    """åŠ è½½è¯„ä¼°æ•°æ®é›†ï¼Œæ”¯æŒjsonå’Œjsonlæ ¼å¼"""
    data = []
    if data_path.endswith('.jsonl'):
        # jsonlæ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªjsonå¯¹è±¡
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    data.append(json.loads(line))
    else:
        # jsonæ ¼å¼ï¼šæ•´ä¸ªæ–‡ä»¶æ˜¯ä¸€ä¸ªjsonæ•°ç»„æˆ–å¯¹è±¡
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    return data

def format_prompt(example):
    """æ ¼å¼åŒ–è¯„ä¼°æç¤ºï¼ˆä¸è®­ç»ƒè„šæœ¬æ ¼å¼ä¸€è‡´ï¼‰"""
    query = example.get("query", "")
    language = example.get("tag", "")
    
    # æ„å»ºå¯¹è¯æ ¼å¼ï¼ˆä¸è®­ç»ƒè„šæœ¬ä¸€è‡´ï¼‰
    prompt = f"""
        ### è¾“å…¥:
        {query}

        ### language:
        {language}
        
        ### è¾“å‡º:
"""
    
    return prompt

def generate_response(model, tokenizer, prompt, device, max_new_tokens=200):
    """ç”Ÿæˆæ¨¡å‹å“åº”"""
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=256)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    # è§£ç å®Œæ•´å“åº”
    full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)
    
    # æå–è¾“å‡ºéƒ¨åˆ†ï¼ˆä» "### è¾“å‡º:" ä¹‹åçš„å†…å®¹ï¼‰
    if "### è¾“å‡º:" in full_response:
        response = full_response.split("### è¾“å‡º:")[-1].strip()
        # ç§»é™¤å¯èƒ½çš„ EOS token
        response = response.replace(tokenizer.eos_token, "").strip()
    else:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡è®°ï¼Œè¿”å›å®Œæ•´å“åº”ï¼ˆå»é™¤è¾“å…¥éƒ¨åˆ†ï¼‰
        response = full_response.replace(prompt.strip(), "").strip()
    
    return response

def evaluate_model(model_path, data_path, device=None, max_new_tokens=200):
    """è¯„ä¼°æ¨¡å‹"""
    # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡ï¼ˆä¸è®­ç»ƒè„šæœ¬ä¸€è‡´ï¼‰
    if device is None:
        if is_intel_mac():
            device = "cpu"
            if hasattr(torch.backends, "mps"):
                torch.backends.mps.enabled = False
            print(f"ğŸ’» æ£€æµ‹åˆ° Intel èŠ¯ç‰‡ï¼Œä½¿ç”¨ CPU è®¾å¤‡è¿›è¡Œè¯„ä¼°")
        elif is_apple_silicon() and hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            device = "mps"
            print(f"ğŸ’» æ£€æµ‹åˆ° Apple Siliconï¼Œä½¿ç”¨ MPS (Metal) è®¾å¤‡è¿›è¡Œè¯„ä¼°")
        else:
            device = "cpu"
            if hasattr(torch.backends, "mps"):
                torch.backends.mps.enabled = False
            print(f"ğŸ’» ä½¿ç”¨ {device} è®¾å¤‡è¿›è¡Œè¯„ä¼°")
    else:
        print(f"ğŸ’» ä½¿ç”¨ {device} è®¾å¤‡è¿›è¡Œè¯„ä¼°")
    
    print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
    
    # åŠ è½½åˆ†è¯å™¨
    print("\næ­£åœ¨åŠ è½½åˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
        padding_side="right"
    )
    
    # è®¾ç½®pad_tokenï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½æ¨¡å‹
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    # æ ¹æ®è®¾å¤‡é€‰æ‹©æ•°æ®ç±»å‹
    if device == "mps":
        dtype = torch.float16
        print("ğŸ“Š ä½¿ç”¨ float16 æ•°æ®ç±»å‹ä»¥èŠ‚çœå†…å­˜")
    else:
        dtype = torch.float32
    
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype=dtype
    )
    
    # ç§»åŠ¨åˆ°è®¾å¤‡
    if device != "cpu":
        model = model.to(device)
    else:
        model = model.cpu()
        # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½åœ¨ CPU ä¸Š
        for param in model.parameters():
            if param.device.type != "cpu":
                param.data = param.data.cpu()
    
    model.eval()
    
    # æ¸…ç†ç¼“å­˜
    if device == "mps":
        torch.mps.empty_cache()
    elif device == "cuda":
        torch.cuda.empty_cache()
    gc.collect()
    
    # åŠ è½½æ•°æ®é›†
    print("æ­£åœ¨åŠ è½½è¯„ä¼°æ•°æ®é›†...")
    data = load_dataset(data_path)
    
    # è¯„ä¼°ç»“æœ
    results = []
    
    print(f"\nå¼€å§‹è¯„ä¼°ï¼Œå…± {len(data)} ä¸ªæ ·æœ¬...")
    
    for i, example in enumerate(data):
        print(f"\n{'='*60}")
        print(f"ğŸ“ è¯„ä¼°æ ·æœ¬ {i+1}/{len(data)}")
        print(f"{'='*60}")
        
        # æ ¼å¼åŒ–æç¤º
        prompt = format_prompt(example)
        query = example.get("query", "")
        language = example.get("tag", "")
        
        print(f"\nğŸ“¥ è¾“å…¥:")
        print(f"  Query: {query}")
        print(f"  Language: {language}")
        
        # ç”Ÿæˆå“åº”
        response = generate_response(model, tokenizer, prompt, device, max_new_tokens=max_new_tokens)
        print(f"\nğŸ¤– æ¨¡å‹è¾“å‡º:")
        print(f"  {response}")
        
        # æœŸæœ›è¾“å‡º
        expected_output = example.get("response", "")
        print(f"\nâœ… æœŸæœ›è¾“å‡º:")
        print(f"  {expected_output}")
        
        # ç®€å•ç›¸ä¼¼åº¦æ£€æŸ¥ï¼ˆå¯é€‰ï¼‰
        similarity = "âœ“" if expected_output.strip() in response or response.strip() in expected_output else "âœ—"
        print(f"\nğŸ“Š åŒ¹é…åº¦: {similarity}")
        
        # ä¿å­˜ç»“æœ
        results.append({
            "query": query,
            "tag": language,
            "expected_response": expected_output,
            "model_response": response,
            "match": similarity == "âœ“"
        })
    
    return results

def save_results(results, output_path):
    """ä¿å­˜è¯„ä¼°ç»“æœ"""
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ“ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_path}")

def calculate_metrics(results):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    total = len(results)
    matched = sum(1 for r in results if r.get("match", False))
    match_rate = (matched / total * 100) if total > 0 else 0
    
    return {
        "total_samples": total,
        "matched_samples": matched,
        "match_rate": f"{match_rate:.2f}%"
    }

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="LLM SFT æ¨¡å‹è¯„ä¼°è„šæœ¬")
    parser.add_argument("--model_path", type=str, default="../outputs/sft_results/final_model", 
                       help="å¾®è°ƒåçš„æ¨¡å‹è·¯å¾„")
    parser.add_argument("--data_path", type=str, default="../data/self_cognition.jsonl", 
                       help="è¯„ä¼°æ•°æ®é›†è·¯å¾„ï¼ˆæ”¯æŒ json å’Œ jsonl æ ¼å¼ï¼‰")
    parser.add_argument("--output_path", type=str, default="../outputs/evaluation_results.json", 
                       help="è¯„ä¼°ç»“æœä¿å­˜è·¯å¾„")
    parser.add_argument("--device", type=str, default=None, 
                       help="ä½¿ç”¨çš„è®¾å¤‡ï¼ˆcpu/mps/cudaï¼‰ï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹")
    parser.add_argument("--max_new_tokens", type=int, default=200, 
                       help="ç”Ÿæˆçš„æœ€å¤§tokenæ•°")
    
    args = parser.parse_args()
    
    # ç¡®ä¿è·¯å¾„æ˜¯ç»å¯¹è·¯å¾„
    if not os.path.isabs(args.model_path):
        args.model_path = os.path.join(os.path.dirname(__file__), args.model_path)
    if not os.path.isabs(args.data_path):
        args.data_path = os.path.join(os.path.dirname(__file__), args.data_path)
    if not os.path.isabs(args.output_path):
        args.output_path = os.path.join(os.path.dirname(__file__), args.output_path)
    
    # è¯„ä¼°æ¨¡å‹
    results = evaluate_model(args.model_path, args.data_path, args.device, args.max_new_tokens)
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = calculate_metrics(results)
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    save_results(results, args.output_path)
    
    # æ‰“å°æ€»ç»“
    print(f"\n{'='*60}")
    print(f"ğŸ“Š è¯„ä¼°æ€»ç»“")
    print(f"{'='*60}")
    print(f"æ€»æ ·æœ¬æ•°: {metrics['total_samples']}")
    print(f"åŒ¹é…æ ·æœ¬æ•°: {metrics['matched_samples']}")
    print(f"åŒ¹é…ç‡: {metrics['match_rate']}")
    print(f"{'='*60}")
    print(f"\nğŸ‰ è¯„ä¼°å®Œæˆï¼")
    print(f"ğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {args.output_path}")

if __name__ == "__main__":
    main()
