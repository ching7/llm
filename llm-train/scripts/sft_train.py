#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM SFT å¾®è°ƒè„šæœ¬
æ”¯æŒ MPS (Metal) å’Œ CPU è¿›è¡Œæ¨¡å‹ç›‘ç£å¾®è°ƒ
è‡ªåŠ¨ä¼˜åŒ–å†…å­˜ä½¿ç”¨ä»¥é€‚é… MPS è®¾å¤‡
"""

import os
import json
import torch
import gc
import platform
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)

def is_apple_silicon():
    """æ£€æµ‹æ˜¯å¦æ˜¯ Apple Silicon (M1/M2/M3 ç­‰)"""
    try:
        # æ£€æŸ¥å¤„ç†å™¨æ¶æ„
        machine = platform.machine()
        # Apple Silicon çš„æ¶æ„æ˜¯ 'arm64'
        if machine == 'arm64':
            # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦æœ‰ MPS æ”¯æŒ
            if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                return True
        return False
    except:
        return False

def is_intel_mac():
    """æ£€æµ‹æ˜¯å¦æ˜¯ Intel Mac"""
    try:
        machine = platform.machine()
        # Intel Mac çš„æ¶æ„æ˜¯ 'x86_64'
        return machine == 'x86_64'
    except:
        return False

# æ£€æµ‹èŠ¯ç‰‡ç±»å‹å¹¶è®¾ç½® MPS
if is_intel_mac():
    # Intel Mac ä¸æ”¯æŒ MPSï¼Œæ˜¾å¼ç¦ç”¨
    if hasattr(torch.backends, "mps"):
        torch.backends.mps.enabled = False
        print("ğŸ”§ Intel Macï¼šå·²ç¦ç”¨ MPS æ”¯æŒ")
elif is_apple_silicon():
    # Apple Silicon å¯ä»¥ä½¿ç”¨ MPSï¼Œè®¾ç½®å†…å­˜é™åˆ¶
    # è®¾ç½® MPS å†…å­˜é™åˆ¶ï¼ˆé™åˆ¶ä¸ºç³»ç»Ÿå†…å­˜çš„ 80%ï¼Œé¿å…å†…å­˜ä¸è¶³ï¼‰
    # å€¼å¿…é¡»åœ¨ 0.0-1.0 ä¹‹é—´ï¼Œ0.8 è¡¨ç¤ºé™åˆ¶ä¸º 80%
    os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.8"

def load_config(config_path):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    # ç¡®ä¿è·¯å¾„æ˜¯ç»å¯¹è·¯å¾„
    if not os.path.isabs(config_path):
        config_path = os.path.join(os.path.dirname(__file__), config_path)
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    # æ‰©å±•æ¨¡å‹è·¯å¾„ä¸­çš„æ³¢æµªå·
    if config["model_path"].startswith("~"):
        config["model_path"] = os.path.expanduser(config["model_path"])
    
    # ç¡®ä¿æ•°æ®è·¯å¾„æ˜¯ç»å¯¹è·¯å¾„
    if not os.path.isabs(config["data_path"]):
        config["data_path"] = os.path.join(os.path.dirname(__file__), config["data_path"])
    
    # ç¡®ä¿è¾“å‡ºç›®å½•æ˜¯ç»å¯¹è·¯å¾„
    if not os.path.isabs(config["output_dir"]):
        config["output_dir"] = os.path.join(os.path.dirname(__file__), config["output_dir"])
    
    return config

def load_dataset(data_path):
    """åŠ è½½è®­ç»ƒæ•°æ®é›†ï¼Œæ”¯æŒjsonå’Œjsonlæ ¼å¼"""
    data = []
    if data_path.endswith('.jsonl'):
        # jsonlæ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªjsonå¯¹è±¡
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    data.append(json.loads(line))
    else:
        # jsonæ ¼å¼ï¼šæ•´ä¸ªæ–‡ä»¶æ˜¯ä¸€ä¸ªjsonæ•°ç»„æˆ–å¯¹è±¡
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    return data

def format_example(example, tokenizer):
    """æ ¼å¼åŒ–è®­ç»ƒæ ·æœ¬"""
    query = example.get("query", "")
    response = example.get("response", "")
    language = example.get("tag", "")

    # æ„å»ºå¯¹è¯æ ¼å¼
    prompt = f"""
        ### è¾“å…¥:
        {query}

        ### language:
        {language}
        
        ### è¾“å‡º:
        {response}
    """
    
    # å®Œæ•´æ–‡æœ¬
    full_text = prompt + tokenizer.eos_token
    
    return {
        "prompt": prompt,
        "full_text": full_text
    }

def tokenize_function(examples, tokenizer):
    """åˆ†è¯å¤„ç†"""
    tokenized = tokenizer(
        examples["full_text"],
        truncation=True,
        max_length=256,  # å‡å°‘åºåˆ—é•¿åº¦ä»¥é™ä½å†…å­˜å ç”¨ï¼ˆä»512é™åˆ°256ï¼‰
        padding="max_length"
    )
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½é…ç½®
    config = load_config("../configs/sft_config.json")
    
    # æ£€æµ‹èŠ¯ç‰‡ç±»å‹å¹¶è®¾ç½®è®¾å¤‡
    if is_intel_mac():
        # Intel Mac ä¸æ”¯æŒ MPSï¼Œå¼ºåˆ¶ä½¿ç”¨ CPU
        device = "cpu"
        # ç¡®ä¿ MPS å·²ç¦ç”¨ï¼ˆåŒé‡ä¿é™©ï¼‰
        if hasattr(torch.backends, "mps"):
            torch.backends.mps.enabled = False
        print(f"ğŸ’» æ£€æµ‹åˆ° Intel èŠ¯ç‰‡ï¼Œä½¿ç”¨ CPU è®¾å¤‡è¿›è¡Œè®­ç»ƒ")
        print(f"â„¹ï¸  Intel Mac ä¸æ”¯æŒ MPS (Metal) åŠ é€Ÿï¼Œä»… Apple Silicon (M1/M2/M3) æ”¯æŒ")
    elif is_apple_silicon() and hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        # Apple Silicon å¯ä»¥ä½¿ç”¨ MPS
        device = "mps"
        print(f"ğŸ’» æ£€æµ‹åˆ° Apple Siliconï¼Œä½¿ç”¨ MPS (Metal) è®¾å¤‡è¿›è¡Œè®­ç»ƒ")
        print(f"âš ï¸  MPS å†…å­˜é™åˆ¶å·²è®¾ç½®ä¸º 80%")
    else:
        # å…¶ä»–æƒ…å†µä½¿ç”¨ CPU
        device = "cpu"
        # ç¡®ä¿ MPS å·²ç¦ç”¨
        if hasattr(torch.backends, "mps"):
            torch.backends.mps.enabled = False
        print(f"ğŸ’» ä½¿ç”¨ {device} è®¾å¤‡è¿›è¡Œè®­ç»ƒ")
    
    # æ¨¡å‹è·¯å¾„
    model_path = config["model_path"]
    print(f"ğŸ“¦ ä½¿ç”¨æ¨¡å‹: {model_path}")
    
    # åŠ è½½åˆ†è¯å™¨
    print("\næ­£åœ¨åŠ è½½åˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
        padding_side="right"
    )
    
    # è®¾ç½®pad_tokenï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½æ¨¡å‹
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    # æ ¹æ®è®¾å¤‡é€‰æ‹©æ•°æ®ç±»å‹ï¼šMPS å¯ä»¥ä½¿ç”¨ float16 èŠ‚çœå†…å­˜
    if device == "mps":
        # MPS æ”¯æŒ float16ï¼Œå¯ä»¥èŠ‚çœçº¦ 50% å†…å­˜
        dtype = torch.float16
        print("ğŸ“Š ä½¿ç”¨ float16 æ•°æ®ç±»å‹ä»¥èŠ‚çœå†…å­˜")
    else:
        dtype = torch.float32
    
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype=dtype  # æ³¨æ„ï¼šè™½ç„¶è­¦å‘Šè¯´å·²å¼ƒç”¨ï¼Œä½†ç›®å‰ä»éœ€è¦ä½¿ç”¨ torch_dtype
    )
    
    # åªæœ‰åœ¨é CPU è®¾å¤‡æ—¶æ‰ç§»åŠ¨æ¨¡å‹
    if device != "cpu":
        model = model.to(device)
    
    # æ¸…ç†ç¼“å­˜
    if device == "mps":
        torch.mps.empty_cache()
    elif device == "cuda":
        torch.cuda.empty_cache()
    gc.collect()
    
    # åŠ è½½æ•°æ®é›†
    print("æ­£åœ¨åŠ è½½æ•°æ®é›†...")
    raw_data = load_dataset(config["data_path"])
    
    # æ ¼å¼åŒ–æ•°æ®
    formatted_data = [format_example(example, tokenizer) for example in raw_data]
    
    # è½¬æ¢ä¸ºDatasetå¯¹è±¡
    dataset = Dataset.from_list(formatted_data)
    
    # åˆ†è¯å¤„ç†
    print("æ­£åœ¨å¤„ç†è®­ç»ƒæ•°æ®...")
    tokenized_dataset = dataset.map(
        lambda examples: tokenize_function(examples, tokenizer),
        batched=True,
        remove_columns=dataset.column_names
    )
    
    # æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False  # éæ©ç è¯­è¨€æ¨¡å‹
    )
    
    # è®­ç»ƒå‚æ•°
    # æ ¹æ®è®¾å¤‡è°ƒæ•´ batch sizeï¼ˆMPS å†…å­˜æœ‰é™ï¼Œä½¿ç”¨æ›´å°çš„ batch sizeï¼‰
    batch_size = config["per_device_train_batch_size"]
    if device == "mps":
        # MPS å†…å­˜æœ‰é™ï¼Œç¡®ä¿ batch size ä¸è¶…è¿‡ 1
        batch_size = min(batch_size, 1)
        print(f"ğŸ“¦ MPS è®¾å¤‡ï¼šbatch size è®¾ç½®ä¸º {batch_size}")
    
    training_args = TrainingArguments(
        output_dir=config["output_dir"],
        overwrite_output_dir=True,
        num_train_epochs=config["num_train_epochs"],
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=4 if device == "mps" else 1,  # MPS ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯å‡å°‘å†…å­˜å³°å€¼
        learning_rate=config["learning_rate"],
        weight_decay=config["weight_decay"],
        logging_dir=os.path.join(config["output_dir"], "logs"),
        logging_steps=config["logging_steps"],
        save_strategy="epoch",
        save_total_limit=2,
        fp16=(device == "mps"),  # MPS å¯ä»¥ä½¿ç”¨ fp16 èŠ‚çœå†…å­˜ï¼ŒCPU ä¸ä½¿ç”¨
        report_to="none",
        dataloader_num_workers=0,
        # MPS å†…å­˜ä¼˜åŒ–é€‰é¡¹
        dataloader_pin_memory=False,  # MPS ä¸éœ€è¦ pin_memory
        remove_unused_columns=True,  # ç§»é™¤æœªä½¿ç”¨çš„åˆ—ä»¥èŠ‚çœå†…å­˜
        # æ˜ç¡®æŒ‡å®šä¸ä½¿ç”¨ MPSï¼ˆå¦‚æœ device æ˜¯ CPUï¼‰
        no_cuda=(device == "cpu"),  # CPU è®­ç»ƒæ—¶ç¦ç”¨ CUDA
    )
    
    # åˆå§‹åŒ–Trainer
    # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    if device == "cpu":
        # å¼ºåˆ¶æ¨¡å‹åœ¨ CPU ä¸Š
        model = model.cpu()
        # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½åœ¨ CPU ä¸Š
        for param in model.parameters():
            if param.device.type != "cpu":
                param.data = param.data.cpu()
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("\nğŸš€ å¼€å§‹SFTå¾®è°ƒè®­ç»ƒ...")
    try:
        trainer.train()
    except RuntimeError as e:
        if "out of memory" in str(e) or "MPS" in str(e):
            print("\nâŒ MPS å†…å­˜ä¸è¶³ï¼")
            print("ğŸ’¡ å»ºè®®ï¼š")
            print("   1. è¿›ä¸€æ­¥å‡å°‘åºåˆ—é•¿åº¦ï¼ˆå½“å‰ 256ï¼‰")
            print("   2. ä½¿ç”¨ CPU è®­ç»ƒï¼ˆè™½ç„¶æ…¢ä½†ç¨³å®šï¼‰")
            print("   3. ä½¿ç”¨ LoRA ç­‰å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•")
            raise
        else:
            raise
    finally:
        # è®­ç»ƒåæ¸…ç†å†…å­˜
        if device == "mps":
            torch.mps.empty_cache()
        elif device == "cuda":
            torch.cuda.empty_cache()
        gc.collect()
    
    # ä¿å­˜æ¨¡å‹
    print("\nğŸ’¾ ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹...")
    trainer.save_model(os.path.join(config["output_dir"], "final_model"))
    tokenizer.save_pretrained(os.path.join(config["output_dir"], "final_model"))
    
    print("\nğŸ‰ SFTå¾®è°ƒè®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜è·¯å¾„: {os.path.join(config['output_dir'], 'final_model')}")

if __name__ == "__main__":
    main()
